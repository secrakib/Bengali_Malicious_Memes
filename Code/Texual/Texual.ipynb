{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('')\n",
    "val_df=pd.read_csv('')\n",
    "test_df=pd.read_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DropOut Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DROPOUT_RATE = 0.8\n",
    "print(f'Drop Out Rate {DROPOUT_RATE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = .01\n",
    "#WEIGHT_DECAY = 0.01\n",
    "print(f'Weight Decay {WEIGHT_DECAY}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "print(f'Seed ={seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256\n",
    "print(f'Max Length {MAX_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "#'''\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "print('Weighted Loss Function')\n",
    "#'''\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#print('Normal Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bangla Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 2\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"sagorsarker/bangla-bert-base\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Bangla Bert')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banglish Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 512\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"csebuetnlp/banglishbert\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Banglish Bert')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel,  get_linear_schedule_with_warmup\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 512\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class XLMRobertaClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(XLMRobertaClassifier, self).__init__()\n",
    "        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use the [CLS] token representation (first token)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "model = XLMRobertaClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Xlm Roberta')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = XLMRobertaClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 2\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"google-bert/bert-base-multilingual-uncased\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('M Bert')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muril Base Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 2\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"google/muril-base-cased\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Muril ')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indic Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 2\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"ai4bharat/indic-bert\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Indic Bert')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDebarta v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "#MAX_LENGTH = 2\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "WARMUP_STEPS = 0\n",
    "#WEIGHT_DECAY = 0.01\n",
    "#DROPOUT_RATE = 0.3\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"  # Changed to XLM-RoBERTa model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Assume we have a DataFrame df with 'text' and 'choice' columns\n",
    "# If you need to load it:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "\n",
    "\n",
    "# Load XLM-RoBERTa tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,use_fast=False)\n",
    "\n",
    "\n",
    "# Custom dataset class for text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataframe.iloc[idx]['text'])  # Ensure text is string\n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        # Tokenize text - Note: XLM-RoBERTa doesn't use token_type_ids\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TextDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = TextDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = TextDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Define XLM-RoBERTa model with classifier\n",
    "class BanglaBERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.3):\n",
    "        print(f'Dropout rate {dropout_rate}')\n",
    "        super(BanglaBERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Use pooler_output for DeBERTa-v3 instead of CLS token\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # DeBERTa-v3 doesn't have pooler_output, use mean pooling or CLS token\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token approach\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = BanglaBERTClassifier(MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "'''# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "class_weights = torch.tensor([\n",
    "    math.log(total_samples / (class_counts[i] + 1e-5)) for i in range(NUM_CLASSES)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "#criterion = nn.CrossEntropyLoss()'''\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Calculate total training steps for scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate_model(model, data_loader, device,name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        # In case of issues with ROC AUC calculation\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss.item()})\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE,'Validating')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'Best Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE,'Testing')\n",
    "\n",
    "print('Indic Bert')\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]} (AUC = {roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i):.4f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'dropout_rate': DROPOUT_RATE\n",
    "    }\n",
    "}, 'xlm_roberta_classifier.pt')\n",
    "\n",
    "# Example of how to load and use the model for inference\n",
    "def load_model_for_inference(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BanglaBERTClassifier(\n",
    "        checkpoint['config']['model_name'],\n",
    "        checkpoint['config']['num_classes'],\n",
    "        checkpoint['config']['dropout_rate']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint['class_to_idx'], checkpoint['idx_to_class'], checkpoint['config']\n",
    "\n",
    "def predict_text(text, model, tokenizer, config, idx_to_class, device=torch.device('cpu')):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=config['max_length'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_class = idx_to_class[preds.item()]\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    return predicted_class, probs.cpu().numpy()[0]\n",
    "\n",
    "# Example usage:\n",
    "'''\n",
    "model, class_to_idx, idx_to_class, config = load_model_for_inference('xlm_roberta_classifier.pt')\n",
    "sample_text = \"আপনার বাংলা টেক্সট এখানে লিখুন\"\n",
    "predicted_class, probabilities = predict_text(sample_text, model, tokenizer, config, idx_to_class)\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7571626,
     "sourceId": 12033598,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
