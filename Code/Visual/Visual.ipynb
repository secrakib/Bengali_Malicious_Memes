{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('')\n",
    "val_df=pd.read_csv('')\n",
    "test_df=pd.read_csv('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "print(f\"Seed = {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "D_O=.8\n",
    "print(f\"Dropout = {D_O}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print('Normal Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested iny\n",
    "target_classes = ['x','y','z']\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "\n",
    "print('Resnet 50')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Net b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 300\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=preprocess.mean, std=preprocess.std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=preprocess.mean, std=preprocess.std)\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize  model\n",
    "weights = EfficientNet_B3_Weights.DEFAULT\n",
    "model = efficientnet_b3(weights=weights)\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Efficient net n3')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vit B_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "model = models.vit_b_16(weights='DEFAULT')  # or 'DEFAULT' for newer torchvision\n",
    "\n",
    "# Freeze or modify if needed\n",
    "num_features = model.heads.head.in_features\n",
    "model.heads.head = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Vit B16')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "model = models.convnext_base(weights='IMAGENET1K_V1')\n",
    "num_features = model.classifier[2].in_features\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(num_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('ConvNext')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_t, Swin_T_Weights,swin_b, Swin_B_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "weights = Swin_B_Weights.IMAGENET1K_V1\n",
    "train_transforms = weights.transforms()\n",
    "val_transforms = weights.transforms()\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "weights = Swin_B_Weights.IMAGENET1K_V1\n",
    "model = swin_b(weights=weights)\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Swin Transformer')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_t, Swin_T_Weights,swin_b, Swin_B_Weights\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights,swin_v2_b, Swin_V2_B_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "train_transforms = weights.transforms()\n",
    "val_transforms = weights.transforms()\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "model = swin_v2_b(weights=weights)\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Swin Transformer V2')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Swin V2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_t, Swin_T_Weights,swin_b, Swin_B_Weights\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights,swin_v2_b, Swin_V2_B_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "#train_transforms = weights.transforms()\n",
    "#val_transforms = weights.transforms()\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "model = swin_v2_b(weights=weights)\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Swin Transformer V2')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SWin \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_t, Swin_T_Weights,swin_b, Swin_B_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from  torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define constants\n",
    "IMAGE_DIR = \"\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 3\n",
    "#D_O=.5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Filter dataframe to only include the classes we're interested in\n",
    "target_classes = ['x','y','z']\n",
    "\n",
    "# Custom dataset class\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        label = self.dataframe.iloc[idx]['class_idx']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define data transformations\n",
    "weights = Swin_B_Weights.IMAGENET1K_V1\n",
    "#train_transforms = weights.transforms()\n",
    "#val_transforms = weights.transforms()\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = MemeDataset(train_df, IMAGE_DIR, transform=train_transforms)\n",
    "val_dataset = MemeDataset(val_df, IMAGE_DIR, transform=val_transforms)\n",
    "test_dataset = MemeDataset(test_df, IMAGE_DIR, transform=val_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize ResNet50 model\n",
    "weights = Swin_B_Weights.IMAGENET1K_V1\n",
    "model = swin_b(weights=weights)\n",
    "model.head = nn.Sequential(\n",
    "    nn.Dropout(D_O),\n",
    "    nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        all_preds_train.extend(preds.cpu().numpy())\n",
    "        all_labels_train.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_f1 = f1_score(all_labels_train, all_preds_train, average='macro')\n",
    "    train_auc = roc_auc_score(all_labels_train, np.eye(NUM_CLASSES)[all_preds_train], multi_class='ovr')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_auc = roc_auc_score(val_labels, np.eye(NUM_CLASSES)[val_preds], multi_class='ovr')\n",
    "    \n",
    "    print(f'Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f} | Train ROC-AUC: {train_auc:.4f}')\n",
    "    print(f'Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping based on validation macro F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'Early stopping Count {no_improve_epochs}')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "print(f'Best Validation f1:{best_f1:.4f}')\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader,desc='Testing'):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "test_auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "print('Swin Transformer')\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_classes, digits=4))\n",
    "print(f'Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr, tpr, _ = roc_curve(np.array(all_labels) == i, np.array(all_preds) == i)\n",
    "    plt.plot(fpr, tpr, label=f'Class {target_classes[i]}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'meme_classifier.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7571626,
     "sourceId": 12033598,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
