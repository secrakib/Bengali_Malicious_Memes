{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss implementation for addressing class imbalance\n",
    "    Standard parameters: alpha=1.0, gamma=2.0\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute cross entropy\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        \n",
    "        # Compute p_t\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing Cross Entropy Loss\n",
    "    Standard parameter: smoothing=0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1, weight=None):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(-1)\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "        \n",
    "        # Create smoothed targets\n",
    "        with torch.no_grad():\n",
    "            smooth_targets = torch.zeros_like(log_probs)\n",
    "            smooth_targets.fill_(self.smoothing / (num_classes - 1))\n",
    "            smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        # Apply class weights if provided\n",
    "        if self.weight is not None:\n",
    "            smooth_targets = smooth_targets * self.weight.unsqueeze(0)\n",
    "        \n",
    "        loss = -smooth_targets * log_probs\n",
    "        return loss.sum(dim=-1).mean()\n",
    "\n",
    "class FocalLossWithLabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Focal Loss with Label Smoothing\n",
    "    Standard parameters: alpha=1.0, gamma=2.0, smoothing=0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, smoothing=0.1, weight=None):\n",
    "        super(FocalLossWithLabelSmoothing, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(-1)\n",
    "        \n",
    "        # Apply label smoothing\n",
    "        with torch.no_grad():\n",
    "            smooth_targets = torch.zeros_like(inputs)\n",
    "            smooth_targets.fill_(self.smoothing / (num_classes - 1))\n",
    "            smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(inputs, dim=-1)\n",
    "        \n",
    "        # Compute cross entropy with smooth targets\n",
    "        ce_loss = -smooth_targets * log_probs\n",
    "        ce_loss = ce_loss.sum(dim=-1)\n",
    "        \n",
    "        # Compute pt for focal loss\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Apply focal loss weighting\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        # Apply class weights if provided\n",
    "        if self.weight is not None:\n",
    "            # Get class weights for each sample based on original targets\n",
    "            sample_weights = self.weight[targets]\n",
    "            focal_loss = focal_loss * sample_weights\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('')\n",
    "val_df = pd.read_csv('')\n",
    "test_df = pd.read_csv('')\n",
    "\n",
    "# Hyperparameters - Optimized for T4 GPU\n",
    "SEED = 42\n",
    "BATCH_SIZE = 4  # Reduced for memory efficiency\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Effective batch size = 4 * 2 = 8\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT_RATE = 0.3\n",
    "MAX_LENGTH = 256   # increased from 128\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 3\n",
    "PATIENCE = 5\n",
    "WARMUP_RATIO = 0.1\n",
    "# bidirectional and unfreezed early layers\n",
    "\n",
    "\n",
    "# CONFIGURATION FLAGS - CHANGE THESE FOR YOUR EXPERIMENTS\n",
    "FREEZE_EARLY_LAYERS = False  # Set to False to unfreeze early layers\n",
    "USE_FOCAL_LOSS = True         # Set to True to use Focal Loss\n",
    "USE_SEPARATE_LOSS= False    \n",
    "USE_LABEL_SMOOTHING = True     # Set to True to use Label Smoothing\n",
    "USE_COMBINED_LOSS = True       # Set to True to use both together\n",
    "CROSS_ATTENTION_HIDDEN_DIM = 512  # Change this for different hidden dimensions (256, 512, 768, 1024)\n",
    "\n",
    "# NEW CROSS-ATTENTION CONFIGURATION VARIABLES\n",
    "TEXT_TO_IMAGE_ATTENTION = True   # Set to True for text attending to image\n",
    "IMAGE_TO_TEXT_ATTENTION = True  # Set to True for image attending to text\n",
    "\n",
    "USE_BIDIRECTIONAL_ATTENTION = True\n",
    "if USE_BIDIRECTIONAL_ATTENTION:  # Set to True for both directions, False for unidirectional\n",
    "    TEXT_TO_IMAGE_ATTENTION = True   \n",
    "    IMAGE_TO_TEXT_ATTENTION = True\n",
    "\n",
    "\n",
    "\n",
    "# Model configurations\n",
    "TEXT_MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "IMAGE_DIR = \"\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Target classes\n",
    "target_classes = ['x', 'y', 'z']\n",
    "\n",
    "# Compute class weights for imbalanced dataset\n",
    "class_counts = Counter(train_df['class_idx'])\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "class_weights = torch.tensor([\n",
    "    total_samples / (num_classes * class_counts[i]) for i in range(num_classes)\n",
    "], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "\n",
    "# LOSS FUNCTION CONFIGURATION - CHANGE #3\n",
    "if USE_SEPARATE_LOSS:\n",
    "    if USE_COMBINED_LOSS:\n",
    "        # Combined focal + label smoothing\n",
    "        text_criterion = FocalLossWithLabelSmoothing(alpha=1.0, gamma=2.0, smoothing=0.1)\n",
    "        visual_criterion = FocalLossWithLabelSmoothing(alpha=1.0, gamma=2.0, smoothing=0.1, weight=class_weights)\n",
    "        print('Using separate combined Focal+LabelSmoothing losses')\n",
    "    elif USE_FOCAL_LOSS:\n",
    "        # Focal loss only\n",
    "        text_criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "        visual_criterion = FocalLoss(alpha=1.0, gamma=2.0, weight=class_weights)\n",
    "        print('Using separate Focal losses')\n",
    "    elif USE_LABEL_SMOOTHING:\n",
    "        # Label smoothing only\n",
    "        text_criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "        visual_criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=class_weights)\n",
    "        print('Using separate Label Smoothing losses')\n",
    "    else:\n",
    "        # Original losses\n",
    "        text_criterion = nn.CrossEntropyLoss()\n",
    "        visual_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print('Using original separate losses')\n",
    "else:\n",
    "    # Single loss configuration\n",
    "    if USE_COMBINED_LOSS:\n",
    "        criterion = FocalLossWithLabelSmoothing(alpha=1.0, gamma=2.0, smoothing=0.1, weight=class_weights)\n",
    "        print('Using combined Focal+LabelSmoothing loss')\n",
    "    elif USE_FOCAL_LOSS:\n",
    "        criterion = FocalLoss(alpha=1.0, gamma=2.0, weight=class_weights)\n",
    "        print('Using Focal loss')\n",
    "    elif USE_LABEL_SMOOTHING:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=class_weights)\n",
    "        print('Using Label Smoothing loss')\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print('Using original weighted CrossEntropy loss')\n",
    "\n",
    "print(f'Loss configuration: Focal={USE_FOCAL_LOSS}, LabelSmoothing={USE_LABEL_SMOOTHING}, Combined={USE_COMBINED_LOSS}')\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=False)\n",
    "\n",
    "# Image transforms\n",
    "weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "image_transforms = weights.transforms()\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, tokenizer, max_length, image_transforms):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_transforms = image_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Process text\n",
    "        text = str(row['text'])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process image\n",
    "        img_name = row['image']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='white')\n",
    "            \n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "        \n",
    "        label = row['class_idx']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"Cross-attention mechanism for multimodal fusion with configurable attention directions\"\"\"\n",
    "    def __init__(self, text_dim, image_dim, hidden_dim=512, \n",
    "                 text_to_image=True, image_to_text=True, bidirectional=True):\n",
    "        super(CrossAttentionLayer, self).__init__()\n",
    "        self.text_dim = text_dim\n",
    "        self.image_dim = image_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.text_to_image = text_to_image\n",
    "        self.image_to_text = image_to_text\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Linear projections\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.image_proj = nn.Linear(image_dim, hidden_dim)\n",
    "        \n",
    "        # Attention weights - adaptive heads based on hidden_dim\n",
    "        num_heads = min(8, hidden_dim // 64)\n",
    "        \n",
    "        # Cross-attention modules based on configuration\n",
    "        if self.text_to_image:\n",
    "            self.text_to_image_attn = nn.MultiheadAttention(\n",
    "                hidden_dim, num_heads=num_heads, dropout=0.1, batch_first=True\n",
    "            )\n",
    "            self.text_ln = nn.LayerNorm(hidden_dim)\n",
    "            ffn_dim = hidden_dim * 2\n",
    "            self.text_ffn = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(ffn_dim, hidden_dim)\n",
    "            )\n",
    "        \n",
    "        if self.image_to_text:\n",
    "            self.image_to_text_attn = nn.MultiheadAttention(\n",
    "                hidden_dim, num_heads=num_heads, dropout=0.1, batch_first=True\n",
    "            )\n",
    "            self.image_ln = nn.LayerNorm(hidden_dim)\n",
    "            ffn_dim = hidden_dim * 2\n",
    "            self.image_ffn = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, ffn_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(ffn_dim, hidden_dim)\n",
    "            )\n",
    "        \n",
    "    def forward(self, text_features, image_features):\n",
    "        # Project to same dimension\n",
    "        text_proj = self.text_proj(text_features)  # [batch, seq_len, hidden_dim]\n",
    "        image_proj = self.image_proj(image_features).unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Initialize outputs\n",
    "        text_output = text_proj\n",
    "        image_output = image_proj.squeeze(1)\n",
    "        \n",
    "        # Cross-attention: text attends to image\n",
    "        if self.text_to_image:\n",
    "            text_attended, _ = self.text_to_image_attn(\n",
    "                query=text_proj,\n",
    "                key=image_proj,\n",
    "                value=image_proj\n",
    "            )\n",
    "            text_attended = self.text_ln(text_attended + text_proj)\n",
    "            text_output = text_attended + self.text_ffn(text_attended)\n",
    "        \n",
    "        # Cross-attention: image attends to text\n",
    "        if self.image_to_text:\n",
    "            image_attended, _ = self.image_to_text_attn(\n",
    "                query=image_proj,\n",
    "                key=text_proj,\n",
    "                value=text_proj\n",
    "            )\n",
    "            image_attended = self.image_ln(image_attended + image_proj)\n",
    "            image_output = (image_attended + self.image_ffn(image_attended)).squeeze(1)\n",
    "        \n",
    "        return text_output, image_output\n",
    "\n",
    "\n",
    "# Replace the MultimodalCoAttentionClassifier class (around line 332) with this:\n",
    "class MultimodalCrossAttentionClassifier(nn.Module):\n",
    "    def __init__(self, text_model_name, num_classes, dropout_rate=0.3):\n",
    "        super(MultimodalCrossAttentionClassifier, self).__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_hidden_size = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Image encoder\n",
    "        self.image_encoder = swin_v2_b(weights=Swin_V2_B_Weights.IMAGENET1K_V1)\n",
    "        # Remove the final classification head\n",
    "        self.image_encoder.head = nn.Identity()\n",
    "        image_hidden_size = 1024  # Swin-V2-B output dimension\n",
    "        \n",
    "        # LAYER FREEZING CONFIGURATION\n",
    "        if FREEZE_EARLY_LAYERS:\n",
    "            print(\"Freezing early layers to save memory...\")\n",
    "            # Freeze early layers of text encoder\n",
    "            for param in self.text_encoder.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            for i in range(6):  # Freeze first 6 layers\n",
    "                for param in self.text_encoder.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "            # Freeze early layers of image encoder\n",
    "            for param in self.image_encoder.features[:4].parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(\"All layers are unfrozen for full training...\")\n",
    "        \n",
    "        # Cross-attention layer with configurable attention directions\n",
    "        self.cross_attention = CrossAttentionLayer(\n",
    "            text_hidden_size, \n",
    "            image_hidden_size, \n",
    "            CROSS_ATTENTION_HIDDEN_DIM,\n",
    "            text_to_image=TEXT_TO_IMAGE_ATTENTION,\n",
    "            image_to_text=IMAGE_TO_TEXT_ATTENTION,\n",
    "            bidirectional=USE_BIDIRECTIONAL_ATTENTION\n",
    "        )\n",
    "        \n",
    "        # Fusion and classification - dimensions adapt to cross-attention hidden dim\n",
    "        fusion_input_dim = CROSS_ATTENTION_HIDDEN_DIM * 2  # text + image features\n",
    "        fusion_hidden_dim = CROSS_ATTENTION_HIDDEN_DIM\n",
    "        \n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, fusion_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fusion_hidden_dim, fusion_hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(fusion_hidden_dim // 2, num_classes)\n",
    "        \n",
    "        # For separate loss computation\n",
    "        if USE_SEPARATE_LOSS:\n",
    "            self.text_classifier = nn.Linear(CROSS_ATTENTION_HIDDEN_DIM, num_classes)\n",
    "            self.image_classifier = nn.Linear(CROSS_ATTENTION_HIDDEN_DIM, num_classes)\n",
    "        \n",
    "        # Print attention configuration\n",
    "        print(f\"Cross-attention configuration:\")\n",
    "        print(f\"  Text-to-Image: {TEXT_TO_IMAGE_ATTENTION}\")\n",
    "        print(f\"  Image-to-Text: {IMAGE_TO_TEXT_ATTENTION}\")\n",
    "        print(f\"  Bidirectional: {USE_BIDIRECTIONAL_ATTENTION}\")\n",
    "        print(f\"  Hidden dimension: {CROSS_ATTENTION_HIDDEN_DIM}\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Text encoding\n",
    "        text_outputs = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        text_features = text_outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "        \n",
    "        # Image encoding\n",
    "        image_features = self.image_encoder(images)  # [batch, hidden_size]\n",
    "        \n",
    "        # Cross-attention\n",
    "        text_attended, image_attended = self.cross_attention(text_features, image_features)\n",
    "        \n",
    "        # Global pooling for text (attention-weighted)\n",
    "        text_mask = attention_mask.unsqueeze(-1).float()\n",
    "        text_pooled = (text_attended * text_mask).sum(dim=1) / text_mask.sum(dim=1)\n",
    "        \n",
    "        # Handle separate loss computation\n",
    "        if USE_SEPARATE_LOSS:\n",
    "            # Individual modality predictions\n",
    "            text_logits = self.text_classifier(text_pooled)\n",
    "            image_logits = self.image_classifier(image_attended)\n",
    "            \n",
    "            # Concatenate features for final prediction\n",
    "            fused_features = torch.cat([text_pooled, image_attended], dim=-1)\n",
    "            fused_features = self.fusion_layer(fused_features)\n",
    "            final_logits = self.classifier(fused_features)\n",
    "            \n",
    "            return final_logits, text_logits, image_logits\n",
    "        else:\n",
    "            # Standard multimodal fusion\n",
    "            fused_features = torch.cat([text_pooled, image_attended], dim=-1)\n",
    "            fused_features = self.fusion_layer(fused_features)\n",
    "            logits = self.classifier(fused_features)\n",
    "            \n",
    "            return logits\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MultimodalDataset(train_df, IMAGE_DIR, tokenizer, MAX_LENGTH, image_transforms)\n",
    "val_dataset = MultimodalDataset(val_df, IMAGE_DIR, tokenizer, MAX_LENGTH, image_transforms)\n",
    "test_dataset = MultimodalDataset(test_df, IMAGE_DIR, tokenizer, MAX_LENGTH, image_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize model\n",
    "model = MultimodalCrossAttentionClassifier(TEXT_MODEL_NAME, NUM_CLASSES, DROPOUT_RATE)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def evaluate_model(model, data_loader, device, name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=name):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            with autocast('cuda'):\n",
    "                if USE_SEPARATE_LOSS:\n",
    "                    final_outputs, text_outputs, image_outputs = model(input_ids, attention_mask, images)\n",
    "                    # Compute losses with configured loss functions\n",
    "                    if USE_COMBINED_LOSS or USE_FOCAL_LOSS or USE_LABEL_SMOOTHING:\n",
    "                        final_loss = criterion(final_outputs, labels) if not USE_SEPARATE_LOSS else text_criterion(final_outputs, labels)\n",
    "                    else:\n",
    "                        final_loss = nn.CrossEntropyLoss()(final_outputs, labels)\n",
    "                    \n",
    "                    text_loss = text_criterion(text_outputs, labels)\n",
    "                    image_loss = visual_criterion(image_outputs, labels)\n",
    "                    loss = 0.5 * final_loss + 0.25 * text_loss + 0.25 * image_loss\n",
    "                    outputs = final_outputs\n",
    "                else:\n",
    "                    outputs = model(input_ids, attention_mask, images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, np.eye(NUM_CLASSES)[all_preds], multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "best_f1 = 0.0\n",
    "no_improve_epochs = 0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            if USE_SEPARATE_LOSS:\n",
    "                final_outputs, text_outputs, image_outputs = model(input_ids, attention_mask, images)\n",
    "                # Compute losses with configured loss functions\n",
    "                if USE_COMBINED_LOSS or USE_FOCAL_LOSS or USE_LABEL_SMOOTHING:\n",
    "                    final_loss = text_criterion(final_outputs, labels)  # Use text_criterion for consistency\n",
    "                else:\n",
    "                    final_loss = nn.CrossEntropyLoss()(final_outputs, labels)\n",
    "                    \n",
    "                text_loss = text_criterion(text_outputs, labels)\n",
    "                image_loss = visual_criterion(image_outputs, labels)\n",
    "                loss = (0.5 * final_loss + 0.25 * text_loss + 0.25 * image_loss) / GRADIENT_ACCUMULATION_STEPS\n",
    "                outputs = final_outputs\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask, images)\n",
    "                loss = criterion(outputs, labels) / GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS * input_ids.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item() * GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"lr\": scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate_model(model, val_loader, DEVICE, 'Validation')\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_f1 > best_f1:\n",
    "        print(f'Validation F1 improved from {best_f1:.4f} to {val_f1:.4f}')\n",
    "        best_f1 = val_f1\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        print(f'No improvement for {no_improve_epochs} epochs')\n",
    "    \n",
    "    if no_improve_epochs >= PATIENCE:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "        \n",
    "print(f'\\nBest Validation F1: {best_f1:.4f}')\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Test evaluation\n",
    "_, test_acc, test_f1, test_auc, test_preds, test_labels = evaluate_model(model, test_loader, DEVICE, 'Testing')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('MULTIMODAL CO-ATTENTION FUSION RESULTS')\n",
    "print('='*60)\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=target_classes, digits=4))\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "print(f'Test F1-Score: {test_f1:.4f}')\n",
    "print(f'Test ROC-AUC: {test_auc:.4f}')\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_classes, yticklabels=target_classes, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# ROC Curves\n",
    "for i in range(NUM_CLASSES):\n",
    "    if len(np.unique(np.array(test_labels) == i)) > 1:  # Check if both classes exist\n",
    "        fpr, tpr, _ = roc_curve(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "        auc_score = roc_auc_score(np.array(test_labels) == i, np.array(test_preds) == i)\n",
    "        axes[1].plot(fpr, tpr, label=f'{target_classes[i]} (AUC = {auc_score:.4f})')\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multimodal_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'text_model_name': TEXT_MODEL_NAME,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'image_size': IMAGE_SIZE\n",
    "    },\n",
    "    'best_f1': best_f1,\n",
    "    'target_classes': target_classes\n",
    "}, 'multimodal_coattention_classifier.pt')\n",
    "\n",
    "print(f\"\\nModel saved as 'multimodal_coattention_classifier.pt'\")\n",
    "print(f\"Best validation F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nTraining completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7571626,
     "sourceId": 12033598,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
